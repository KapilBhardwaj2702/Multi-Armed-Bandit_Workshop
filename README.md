# ğŸ° Casino Challenge â€” Multi-Armed Bandits & Îµ-Greedy (Gamified Workshop)

## ğŸ“˜ Overview
This workshop explores the **explorationâ€“exploitation trade-off** in Reinforcement Learning using a fun and competitive **Multi-Armed Bandit (MAB)** challenge.

Students implement **Îµ-greedy policies**, test them under **stationary** and **non-stationary** conditions, and analyze their strategies through automatically generated results.  
Each run produces CSV files for leaderboard comparison and reflection.

---

## ğŸ§  Learning Objectives
By completing this workshop, students will be able to:

- Explain the **exploration vs. exploitation** dilemma.  
- Implement **Îµ-greedy** and **decaying Îµ-greedy** algorithms.  
- Compare the effects of different Îµ values on total performance.  
- Adapt policies for **non-stationary bandits** using a constant step-size (Î±).  
- Reflect on their learning strategy through saved results and visualizations.

---

## ğŸ—ï¸ Workshop Structure

| Phase | Activity | Description |
|:------|:----------|:-------------|
| **1** | Setup & Introduction | Review MAB concepts and workshop goals. |
| **2** | Round 1 â€“ Stationary Casino | Implement Îµ-greedy to maximize reward in a fixed environment. |
| **3** | Leaderboard & Reflection | Submit results, view rankings, and analyze strategies. |
| **4** | Round 2 â€“ Non-Stationary Casino | Adapt to drifting reward probabilities using constant Î±. |
| **5** | Final Discussion | Connect results to real-world examples (ads, A/B testing, recommendations). |

---

## ğŸ® Gamified Components

- **Leaderboards:**  
  Each run produces two result files:
  - `casino_round1_results.csv` â€” Stationary bandit round  
  - `casino_round2_results.csv` â€” Non-stationary bandit round  

  These CSVs can be aggregated by the instructor for leaderboard visualization.

- **Badges / Awards:**
  - ğŸ¥‡ **Efficient Exploiter** â€” Highest reward with low Îµ  
  - ğŸ§­ **Risk Taker** â€” High Îµ with competitive performance  
  - ğŸ”„ **Adaptive Strategist** â€” Best performance in non-stationary environment  

- **Reflection Prompts:**  
  Encourage analysis of how exploration and adaptability affect long-term success.

---

## ğŸ’» Technical Notes

- **Compatible Environments:** Jupyter Notebook, Google Colab  
- **Dependencies:** `numpy`, `matplotlib`, `pandas`, `IPython.display`  
- **Outputs:** Automatically saves CSVs for both rounds:
  - `casino_round1_results.csv`
  - `casino_round2_results.csv`
- **Automation:**  
  The added code at the end of the notebook executes both rounds and saves the corresponding result files automatically.  
  These CSVs are then used for leaderboard comparison and analysis.

---

## ğŸ§© Files Included

| File | Description |
|:------|:-------------|
| `Muli-Armed Bandit_Workshop.ipynb` | Main notebook with both rounds, code, and reflection cells. |
| `README.md` | Workshop summary and documentation. |
| `casino_round1_results.csv` | Output CSV for Round 1 (stationary environment). |
| `casino_round2_results.csv` | Output CSV for Round 2 (non-stationary environment). |

---

## ğŸ§­ Instructor Tips

- Keep the same random seed (`SEED_ENV`) across students for fairness.  
- Hide true means during competition to preserve realism.  
- Encourage explanation of why certain Îµ values performed better.  
- Optionally extend with **UCB**, **Thompson Sampling**, or **softmax exploration** for advanced learners.

---

## ğŸ’¬ Example Discussion Points

1. **Stability vs. Adaptability:** Round 1 favored exploitation in a stable environment, while Round 2 required flexibility.  
2. **Explorationâ€™s Hidden Value:** A fixed Îµ may fail when the world changes â€” adaptive exploration is key.  
3. **Integration Update:** The final code automatically generates both CSVs and uses them for direct result comparison and visualization.
